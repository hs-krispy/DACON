## Review(느낀점)

##### 2020.10.12 ~ 2020.10.25까지 진행된 딥러닝/클라우드 과목의 Classification 경진대회 후기



제출결과(50% 채점 결과) **0.9290**

제출결과(100% 채점 결과) **0.921986 (7th)**



이번 경진 대회는 Dacon의 천제 분류 대회 이후에 2번째 Classification 경진대회 경험이었습니다.

이번 경진 대회를 진행하면서 가장 크게 느꼈던 점은 이러한 경진대회나 머신러닝을 다루는 부분은 경험적인 부분이 정말 큰 영향을 미친다는 것(최근에 Dacon의 천제 분류 대회를 진행했던 것이 어느 정도 도움이 되었습니다.)과 내 생각과 제출시에 나오는 결과(test set 50% 만 채점)가 많이 다르다는 부분이었습니다.

우선 Validation 정확도가 높게 나와도 실제 제출결과는 그렇지 않았던 부분이 가장 의아하면서 이유가 궁금했는데 이는 최종 결과(test set 100% 채점)를 보고나니 50% 만 채점해서 비교적 정확하지 않은 그러한 결과가 나온 것 같다는 생각이 들었습니다.

또한 smote를 진행할 때에는 cross_validation으로 검증하기 전에 전체 train data set에 적용하는 것이 아니라 train set을 kfold를 이용해 train, validation set으로 나누고 이때 train set에 적용하는 것으로 알고 있었는데 막상 전체 train data에 smote를 적용한 제출결과가 가장 잘 나온 것(test set 50% 만 채점 시)을 보면 smote를 진행할 때에는 항상 cross_validation 안에서 적용해야하는 것이 옳은 것 인가에 대한 궁금증도 생겼습니다. (이 부분은 좀 더 알아보아야 할 것 같습니다.)

그리고 feature selection 기법을 통해 불필요하다고 여겨지는 feature들을 골라내거나 기존의 피처들을 이용한 파생변수를 추가하는 것이 기존의 data를 가지고 학습하는 것보다 항상 좋은 성능을 보이는 것만은 아니라는 것도 깨닫게 되었습니다.

stacking이나 voting 같은 경우에는 처음 사용해보는 방식이었는데 제대로 알고 사용한 것이 아니라서 효율적으로 사용하지 못한 것 같습니다.  조금 더 많은 분류기를 이용해서 구성해보면 어땠을까 하는 생각이 들었고 파생변수를 추가하는 방식을 사용해보면서 계속 변수들을 바꿔가는 과정속에서 gridsearch 등 parameter tuning 과정을 효과적으로 진행하지 못한 부분이 가장 큰 아쉬움이 남았습니다. 

하지만 이번 대회를 통해 BayesSearch, stacking, voting 등 처음 사용해보는 방식들을 접해볼 수 있었던 것은 좋은 경험이었던 것 같습니다.

앞으로 기회가 된다면 이러한 대회들을 통해 경험을 많이 쌓고 싶고 그 경험을 토대로 조금 더 공부하고 발전해 나갈수 있었으면 좋겠다는 생각이 들었습니다. 특히, feature나 parameter를 tuning하는 데에 있어 좀 더 신중을 기할 필요가 있다는 생각이 들었습니다.